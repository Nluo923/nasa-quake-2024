{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import obspy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Miniseed data (seismic data)\n",
    "# Make sure the path points to a real file in your data folder\n",
    "miniseed_file_path = 'data/lunar/training/data/S12_GradeA/xa.s12.00.mhz.1970-01-19HR00_evid00002.mseed'  # Assuming miniseed extension\n",
    "\n",
    "# Read the miniseed file using obspy (this is only for miniseed format files)\n",
    "try:\n",
    "    st = obspy.read(miniseed_file_path)\n",
    "    st.plot()  # Plot seismic trace to visualize data\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "\n",
    "# For CSV data (if you're working with CSVs)\n",
    "csv_file_path = 'data/lunar/training/data/S12_GradeA/xa.s12.00.mhz.1970-01-19HR00_evid00002.csv'  # Check the correct path and file extension\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(df.head())  # Display the first few rows to confirm the data\n",
    "    df.plot()  # Plot to visualize any trends or anomalies\n",
    "    plt.show()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a bandpass filter between 0.5 Hz and 10 Hz, which is commonly used for seismic data\n",
    "filtered_data = st.copy()\n",
    "filtered_data.filter('bandpass', freqmin=0.5, freqmax=3)\n",
    "filtered_data.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)  # Display the available columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'time_abs(%Y-%m-%dT%H:%M:%S.%f)' column to datetime format\n",
    "df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'] = pd.to_datetime(df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'], format='%Y-%m-%dT%H:%M:%S.%f')\n",
    "\n",
    "# Check if the conversion was successful by printing the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot velocity over time\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'], df['velocity(m/s)'], color='blue')\n",
    "plt.title('Lunar Seismic Velocity Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple rolling mean to smooth the data\n",
    "df['velocity_smoothed'] = df['velocity(m/s)'].rolling(window=10).mean()\n",
    "\n",
    "# Plot smoothed velocity over time\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'], df['velocity_smoothed'], color='red')\n",
    "plt.title('Smoothed Lunar Seismic Velocity Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Smoothed Velocity (m/s)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Feature selection (we will use time_rel(sec) as our feature and velocity(m/s) as target)\n",
    "X = df[['time_rel(sec)']]  # Features\n",
    "y = df['velocity(m/s)']    # Target\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data using StandardScaler (good practice for models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error and R^2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs predicted values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Values')\n",
    "plt.scatter(X_test, y_pred, color='red', label='Predicted Values', alpha=0.5)\n",
    "plt.title('Actual vs Predicted Seismic Velocity')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted Value is always 0, need to figure out why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the variance in velocity(m/s)\n",
    "Let's check if the velocity(m/s) values are small, which might explain why the model is predicting close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the variance of the velocity column\n",
    "print(\"Mean of velocity(m/s):\", df['velocity(m/s)'].mean())\n",
    "print(\"Variance of velocity(m/s):\", df['velocity(m/s)'].var())\n",
    "print(\"Minimum value of velocity(m/s):\", df['velocity(m/s)'].min())\n",
    "print(\"Maximum value of velocity(m/s):\", df['velocity(m/s)'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Target Variable (velocity(m/s)) and Train the Model Again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the target variable (velocity)\n",
    "scaler = StandardScaler()  # Using StandardScaler to normalize both features and target\n",
    "\n",
    "# Scaling features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Scaling the target variable (velocity)\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))  # Scaling target\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Train the Random Forest model on the scaled data\n",
    "rf_model.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scaled = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the predicted values to return them to original scale\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model again\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Check predicted values\n",
    "print(\"First few predicted values:\", y_pred[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Acceleration as a Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate acceleration (change in velocity over time)\n",
    "df['acceleration'] = df['velocity(m/s)'].diff() / df['time_rel(sec)'].diff()\n",
    "\n",
    "# Drop any NaN values introduced by the diff function\n",
    "df = df.dropna()\n",
    "\n",
    "# Update features (X) and target (y) to include the new acceleration feature\n",
    "X = df[['time_rel(sec)', 'acceleration']]\n",
    "y = df['velocity(m/s)']\n",
    "\n",
    "# Split the dataset into training and testing sets again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features and target again\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scaled = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the predictions back to the original scale\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model again\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(\"First few predicted values:\", y_pred[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZING IT TO MAKE SENSE HOPEFULLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Scatter plot of the relationship between time_rel and velocity\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=df, x='time_rel(sec)', y='velocity(m/s)')\n",
    "plt.title('Velocity vs Time Relative')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR REGRESSION (USE THIS SINCE IT GOT BEST RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scaled = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Inverse transform the predictions back to the original scale\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model again\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(\"First few predicted values:\", y_pred[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRYNA USE ALL THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'time_abs(%Y-%m-%dT%H:%M:%S.%f)' and 'time_rel(sec)' are relevant features\n",
    "X = df[['time_abs(%Y-%m-%dT%H:%M:%S.%f)', 'time_rel(sec)']]  # Add more features if available\n",
    "y = df['velocity(m/s)']\n",
    "\n",
    "# Convert 'time_abs' to datetime and then to integer safely\n",
    "X.loc[:, 'time_abs(%Y-%m-%dT%H:%M:%S.%f)'] = pd.to_datetime(X['time_abs(%Y-%m-%dT%H:%M:%S.%f)']).astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model (you can choose any model, here is SVR)\n",
    "model = SVR(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Miniseed data (seismic data)\n",
    "# Make sure the path points to a real file in your data folder\n",
    "miniseed_file_path = 'data/lunar/training/data/S12_GradeA/xa.s12.00.mhz.1970-01-19HR00_evid00002.mseed'  # Assuming miniseed extension\n",
    "\n",
    "# Read the miniseed file using obspy (this is only for miniseed format files)\n",
    "try:\n",
    "    st = obspy.read(miniseed_file_path)\n",
    "    st.plot()  # Plot seismic trace to visualize data\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "\n",
    "# For CSV data (if you're working with CSVs)\n",
    "csv_file_path = 'data/lunar/training/data/S12_GradeA/xa.s12.00.mhz.1970-01-19HR00_evid00002.csv'  # Check the correct path and file extension\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(df.head())  # Display the first few rows to confirm the data\n",
    "    df.plot()  # Plot to visualize any trends or anomalies\n",
    "    plt.show()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "# Step 1: Define a threshold for seismic activity based on velocity\n",
    "threshold = 1e-9  # Adjust this based on domain knowledge or exploratory data analysis\n",
    "\n",
    "# Step 2: Create a new column 'seismic_activity'\n",
    "df['seismic_activity'] = (df['velocity(m/s)'].abs() > threshold).astype(int)\n",
    "\n",
    "# Step 3: Check the distribution of the labels\n",
    "print(df['seismic_activity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 1: Convert the datetime column to numeric format (timestamps in seconds)\n",
    "df['time_abs_numeric'] = pd.to_datetime(df['time_abs(%Y-%m-%dT%H:%M:%S.%f)']).astype(int) / 10**9  # Convert to seconds\n",
    "\n",
    "# Step 2: Define features (X) and target (y)\n",
    "y = df['seismic_activity']\n",
    "X = df.drop(columns=['seismic_activity', 'time_abs(%Y-%m-%dT%H:%M:%S.%f)'])  # Drop original datetime column\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Step 7: Output the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 7: Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 8: Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Step 9: Display the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Optional: Display confusion matrix and classification report for detailed evaluation\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
