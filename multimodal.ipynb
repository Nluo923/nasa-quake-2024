{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "# 1. Define the CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Instantiate the callback\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[loss_callback])\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Scale the CNN features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# 5. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "y_pred_scaled = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 7. Evaluate the SVR model\n",
    "mse = mean_squared_error(y_test, y_pred_scaled)\n",
    "r2 = r2_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"SVR Mean Squared Error: {mse}\")\n",
    "print(f\"SVR R^2 Score: {r2}\")\n",
    "print(\"First few predicted values:\", y_pred_scaled[:10])\n",
    "\n",
    "# Optional: Visualize predictions vs true values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Arrival Times', color='blue')\n",
    "plt.plot(y_pred_scaled, label='Predicted Arrival Times', color='red')\n",
    "plt.title('SVR Predictions vs True Values')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Arrival Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINED (MULTI-MODAL SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "# Example: Loading data (replace this with your actual data loading code)\n",
    "x_train = np.random.rand(1000, 100, 1)  # 1000 samples, 100 time steps, 1 feature\n",
    "y_train = np.random.rand(1000)           # 1000 target values\n",
    "x_test = np.random.rand(200, 100, 1)     # 200 samples, 100 time steps, 1 feature\n",
    "y_test = np.random.rand(200)              # 200 target values\n",
    "\n",
    "# 1. Define the CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Instantiate the callback\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[loss_callback])\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Scale the CNN features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# 5. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set using SVR\n",
    "y_pred_svr = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 7. Make predictions on the test set using CNN\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# 8. Combine predictions from both models by averaging\n",
    "final_predictions = (y_pred_svr + y_pred_cnn.flatten()) / 2\n",
    "\n",
    "# 9. Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# 10. Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "\n",
    "# 11. Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "r2_combined = r2_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"R^2 Score: {r2_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"R^2 Score: {r2_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"R^2 Score: {r2_combined}\")\n",
    "\n",
    "# Optional: Visualize predictions vs true values for combined model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Arrival Times', color='blue')\n",
    "plt.plot(final_predictions, label='Combined Predictions', color='red')\n",
    "plt.title('Combined Model Predictions vs True Values')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Arrival Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Loading data (replace this with your actual data loading code)\n",
    "x_train = np.random.rand(1000, 100, 1)  # 1000 samples, 100 time steps, 1 feature\n",
    "y_train = np.random.rand(1000)           # 1000 target values\n",
    "x_test = np.random.rand(200, 100, 1)     # 200 samples, 100 time steps, 1 feature\n",
    "y_test = np.random.rand(200)              # 200 target values\n",
    "\n",
    "# 1. Define the CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Instantiate the callback\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[loss_callback])\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Feature Engineering\n",
    "def create_features(data):\n",
    "    features = pd.DataFrame()\n",
    "    features['mean'] = data.mean(axis=1)\n",
    "    features['std'] = data.std(axis=1)\n",
    "    features['max'] = data.max(axis=1)\n",
    "    features['min'] = data.min(axis=1)\n",
    "    features['median'] = np.median(data, axis=1)\n",
    "    return features\n",
    "\n",
    "# Generate new features\n",
    "train_new_features = create_features(train_features)\n",
    "test_new_features = create_features(test_features)\n",
    "\n",
    "# 5. Scale the new features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_new_features)\n",
    "test_features_scaled = scaler.transform(test_new_features)\n",
    "\n",
    "# 6. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 7. Make predictions on the test set using SVR\n",
    "y_pred_svr = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 8. Make predictions on the test set using CNN\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# 9. Combine predictions from both models by averaging\n",
    "final_predictions = (y_pred_svr + y_pred_cnn.flatten()) / 2\n",
    "\n",
    "# 10. Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# 11. Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "\n",
    "# 12. Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "r2_combined = r2_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"R^2 Score: {r2_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"R^2 Score: {r2_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"R^2 Score: {r2_combined}\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "\n",
    "# Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "rmse_svr = np.sqrt(mse_svr)\n",
    "mape_svr = mean_absolute_percentage_error(y_test, y_pred_svr)\n",
    "explained_variance_svr = explained_variance_score(y_test, y_pred_svr)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "mae_cnn = mean_absolute_error(y_test, y_pred_cnn)\n",
    "rmse_cnn = np.sqrt(mse_cnn)\n",
    "mape_cnn = mean_absolute_percentage_error(y_test, y_pred_cnn)\n",
    "explained_variance_cnn = explained_variance_score(y_test, y_pred_cnn)\n",
    "\n",
    "# Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "mae_combined = mean_absolute_error(y_test, final_predictions)\n",
    "rmse_combined = np.sqrt(mse_combined)\n",
    "mape_combined = mean_absolute_percentage_error(y_test, final_predictions)\n",
    "explained_variance_combined = explained_variance_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"Mean Absolute Error: {mae_svr}\")\n",
    "print(f\"Root Mean Squared Error: {rmse_svr}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape_svr * 100}%\")\n",
    "print(f\"Explained Variance Score: {explained_variance_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"Mean Absolute Error: {mae_cnn}\")\n",
    "print(f\"Root Mean Squared Error: {rmse_cnn}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape_cnn * 100}%\")\n",
    "print(f\"Explained Variance Score: {explained_variance_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"Mean Absolute Error: {mae_combined}\")\n",
    "print(f\"Root Mean Squared Error: {rmse_combined}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape_combined * 100}%\")\n",
    "print(f\"Explained Variance Score: {explained_variance_combined}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - increased depth by adding more convolutional and dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# 1. Define the updated CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second convolutional block\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Third convolutional block\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fourth convolutional block\n",
    "    x = Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)  # More dropout to prevent overfitting\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Instantiate the callback\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[loss_callback])\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Scale the CNN features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# 5. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set using SVR\n",
    "y_pred_svr = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 7. Make predictions on the test set using CNN\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# 8. Combine predictions from both models by averaging\n",
    "final_predictions = (y_pred_svr + y_pred_cnn.flatten()) / 2\n",
    "\n",
    "# 9. Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "explained_variance_svr = explained_variance_score(y_test, y_pred_svr)\n",
    "\n",
    "# 10. Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "explained_variance_cnn = explained_variance_score(y_test, y_pred_cnn)\n",
    "\n",
    "# 11. Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "r2_combined = r2_score(y_test, final_predictions)\n",
    "explained_variance_combined = explained_variance_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"R^2 Score: {r2_svr}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"R^2 Score: {r2_cnn}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"R^2 Score: {r2_combined}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_combined}\")\n",
    "\n",
    "# Optional: Visualize predictions vs true values for combined model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Arrival Times', color='blue')\n",
    "plt.plot(final_predictions, label='Combined Predictions', color='red')\n",
    "plt.title('Combined Model Predictions vs True Values')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Arrival Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# 1. Define the updated CNN model with more layers\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second convolutional block\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Third convolutional block\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fourth convolutional block\n",
    "    x = Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fifth convolutional block\n",
    "    x = Conv1D(512, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)  # More dropout to prevent overfitting\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the updated CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Instantiate the callback\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[loss_callback])\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Scale the CNN features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# 5. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set using SVR\n",
    "y_pred_svr = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 7. Make predictions on the test set using CNN\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# 8. Combine predictions from both models by averaging\n",
    "final_predictions = (y_pred_svr + y_pred_cnn.flatten()) / 2\n",
    "\n",
    "# 9. Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "explained_variance_svr = explained_variance_score(y_test, y_pred_svr)\n",
    "\n",
    "# 10. Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "explained_variance_cnn = explained_variance_score(y_test, y_pred_cnn)\n",
    "\n",
    "# 11. Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "r2_combined = r2_score(y_test, final_predictions)\n",
    "explained_variance_combined = explained_variance_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"R^2 Score: {r2_svr}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"R^2 Score: {r2_cnn}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"R^2 Score: {r2_combined}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_combined}\")\n",
    "\n",
    "# Optional: Visualize predictions vs true values for combined model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Arrival Times', color='blue')\n",
    "plt.plot(final_predictions, label='Combined Predictions', color='red')\n",
    "plt.title('Combined Model Predictions vs True Values')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Arrival Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVEN MORE LAYERS - BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "\n",
    "# 1. Define the updated CNN model with more layers and improved regularization\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)  # Adjusted dropout rate\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second convolutional block\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Third convolutional block\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fourth convolutional block\n",
    "    x = Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fifth convolutional block\n",
    "    x = Conv1D(512, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)  # More dropout to prevent overfitting\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Additional dense layer\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the updated CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 50:  # Start reducing the learning rate after 50 epochs\n",
    "        return lr * 0.5  # Reduce learning rate by 50%\n",
    "    return lr\n",
    "\n",
    "# Instantiate callbacks\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the model with the custom callbacks\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, \n",
    "          callbacks=[loss_callback, early_stopping, lr_scheduler])\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Scale the CNN features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# 5. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set using SVR\n",
    "y_pred_svr = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 7. Make predictions on the test set using CNN\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# 8. Combine predictions from both models by averaging\n",
    "final_predictions = (y_pred_svr + y_pred_cnn.flatten()) / 2\n",
    "\n",
    "# 9. Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "explained_variance_svr = explained_variance_score(y_test, y_pred_svr)\n",
    "\n",
    "# 10. Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "explained_variance_cnn = explained_variance_score(y_test, y_pred_cnn)\n",
    "\n",
    "# 11. Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "r2_combined = r2_score(y_test, final_predictions)\n",
    "explained_variance_combined = explained_variance_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"R^2 Score: {r2_svr}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"R^2 Score: {r2_cnn}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"R^2 Score: {r2_combined}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_combined}\")\n",
    "\n",
    "# Optional: Visualize predictions vs true values for combined model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Arrival Times', color='blue')\n",
    "plt.plot(final_predictions, label='Combined Predictions', color='red')\n",
    "plt.title('Combined Model Predictions vs True Values')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Arrival Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more epochs that didn't really work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "\n",
    "# 1. Define the updated CNN model with more layers and improved regularization\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)  # Adjusted dropout rate\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second convolutional block\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Third convolutional block\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fourth convolutional block\n",
    "    x = Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fifth convolutional block\n",
    "    x = Conv1D(512, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)  # More dropout to prevent overfitting\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Additional dense layer\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the updated CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 50:  # Start reducing the learning rate after 50 epochs\n",
    "        return lr * 0.5  # Reduce learning rate by 50%\n",
    "    return lr\n",
    "\n",
    "# Instantiate callbacks\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the model with the custom callbacks\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, \n",
    "          callbacks=[loss_callback, early_stopping, lr_scheduler])\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Scale the CNN features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# 5. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set using SVR\n",
    "y_pred_svr = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 7. Make predictions on the test set using CNN\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# 8. Combine predictions from both models by averaging\n",
    "final_predictions = (y_pred_svr + y_pred_cnn.flatten()) / 2\n",
    "\n",
    "# 9. Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "explained_variance_svr = explained_variance_score(y_test, y_pred_svr)\n",
    "\n",
    "# 10. Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "explained_variance_cnn = explained_variance_score(y_test, y_pred_cnn)\n",
    "\n",
    "# 11. Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "r2_combined = r2_score(y_test, final_predictions)\n",
    "explained_variance_combined = explained_variance_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"R^2 Score: {r2_svr}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"R^2 Score: {r2_cnn}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"R^2 Score: {r2_combined}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_combined}\")\n",
    "\n",
    "# Optional: Visualize predictions vs true values for combined model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Arrival Times', color='blue')\n",
    "plt.plot(final_predictions, label='Combined Predictions', color='red')\n",
    "plt.title('Combined Model Predictions vs True Values')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Arrival Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nah this next one is mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# 1. Define the updated CNN model with more layers and improved regularization\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)  \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second convolutional block\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Third convolutional block\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fourth convolutional block\n",
    "    x = Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fifth convolutional block\n",
    "    x = Conv1D(512, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)  \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Additional dense layer\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(1)(x)  # Assuming a regression task\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# 2. Create and train the updated CNN model\n",
    "input_shape = x_train.shape[1:]  # Ensure x_train is defined before this line\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Custom callback to print loss after each epoch\n",
    "def print_loss(epoch, logs):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {logs['loss']}\")\n",
    "\n",
    "# Learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 50:  # Start reducing the learning rate after 50 epochs\n",
    "        return lr * 0.5  # Reduce learning rate by 50%\n",
    "    return lr\n",
    "\n",
    "# Instantiate callbacks\n",
    "loss_callback = LambdaCallback(on_epoch_end=print_loss)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the model with the custom callbacks\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, \n",
    "                    callbacks=[loss_callback, early_stopping, lr_scheduler], verbose=1)\n",
    "\n",
    "# 3. Use the CNN model to generate features\n",
    "train_features = model.predict(x_train)\n",
    "test_features = model.predict(x_test)\n",
    "\n",
    "# 4. Scale the CNN features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# 5. Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # You can also experiment with 'linear' or 'poly' kernels\n",
    "svr_model.fit(train_features_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set using SVR\n",
    "y_pred_svr = svr_model.predict(test_features_scaled)\n",
    "\n",
    "# 7. Make predictions on the test set using CNN\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# 8. Combine predictions from both models by averaging\n",
    "final_predictions = (y_pred_svr + y_pred_cnn.flatten()) / 2\n",
    "\n",
    "# 9. Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "explained_variance_svr = explained_variance_score(y_test, y_pred_svr)\n",
    "\n",
    "# 10. Evaluate the CNN model\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "explained_variance_cnn = explained_variance_score(y_test, y_pred_cnn)\n",
    "\n",
    "# 11. Evaluate the combined predictions\n",
    "mse_combined = mean_squared_error(y_test, final_predictions)\n",
    "r2_combined = r2_score(y_test, final_predictions)\n",
    "explained_variance_combined = explained_variance_score(y_test, final_predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVR Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_svr}\")\n",
    "print(f\"R^2 Score: {r2_svr}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_svr}\")\n",
    "\n",
    "print(\"\\nCNN Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_cnn}\")\n",
    "print(f\"R^2 Score: {r2_cnn}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_cnn}\")\n",
    "\n",
    "print(\"\\nCombined Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_combined}\")\n",
    "print(f\"R^2 Score: {r2_combined}\")\n",
    "print(f\"Explained Variance Score: {explained_variance_combined}\")\n",
    "\n",
    "# Optional: Visualize predictions vs true values for combined model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Values', color='blue')\n",
    "plt.plot(final_predictions, label='Combined Predictions', color='red')\n",
    "plt.title('Combined Model Predictions vs True Values')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Target Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
